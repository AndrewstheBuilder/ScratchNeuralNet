{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Implementation for Make_more exercises\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "words=open(\"C:/Users/andre/makemore/names.txt\",'r').read().splitlines()\n",
    "trainLen = int(len(words)*0.8)\n",
    "devLen = int(len(words)*0.1)\n",
    "testLen = len(words)-trainLen-devLen\n",
    "# print('trainLen, devLen, testLen',trainLen, devLen, testLen)\n",
    "devSet = words[:devLen]\n",
    "trainSet = words[devLen:devLen+trainLen]\n",
    "testSet = words[devLen+trainLen:]\n",
    "# if(len(trainSet) == trainLen and len(devSet) == devLen and len(testSet) == testLen):\n",
    "#     print(True)\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  157286\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "xs, ys = [],[]\n",
    "for w in trainSet:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        xs.append((ix1,ix2))\n",
    "        ys.append(ix3)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = int(xs.nelement()/2)\n",
    "print('number of examples: ', num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init neural net\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27*2,27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits.shape torch.Size([157286, 27])\n"
     ]
    }
   ],
   "source": [
    "# index into W instead of doing 'xenc@W'\n",
    "# this solution makes everything slow!!!\n",
    "\n",
    "# logits_list = []  # create a list to hold the individual tensors\n",
    "# for chs in xs:\n",
    "#     firstRow = W[chs[0]]\n",
    "#     secRow = W[chs[1]+27]\n",
    "#     logits_list.append(firstRow + secRow)  # append the new tensor to the list\n",
    "# logits = torch.stack(logits_list)  # concatenate the list of tensors along a new dimension\n",
    "# print('logits.shape',logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([314572])\n",
      "torch.Size([157286, 2])\n"
     ]
    }
   ],
   "source": [
    "x = xs.flatten()\n",
    "print(x.shape)\n",
    "print(xs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2394192218780518\n",
      "2.2394118309020996\n",
      "2.2394044399261475\n",
      "2.2393972873687744\n",
      "2.2393903732299805\n",
      "2.2393834590911865\n",
      "2.2393765449523926\n",
      "2.2393693923950195\n",
      "2.2393624782562256\n",
      "2.2393558025360107\n",
      "2.239348888397217\n",
      "2.239342212677002\n",
      "2.239335536956787\n",
      "2.2393290996551514\n",
      "2.2393226623535156\n",
      "2.239315986633301\n",
      "2.239309310913086\n",
      "2.2393033504486084\n",
      "2.2392966747283936\n",
      "2.239290475845337\n",
      "2.2392845153808594\n",
      "2.2392783164978027\n",
      "2.239272117614746\n",
      "2.2392656803131104\n",
      "2.239259719848633\n",
      "2.2392537593841553\n",
      "2.2392477989196777\n",
      "2.2392420768737793\n",
      "2.239236354827881\n",
      "2.2392303943634033\n",
      "2.239224433898926\n",
      "2.2392187118530273\n",
      "2.239212989807129\n",
      "2.2392075061798096\n",
      "2.239201784133911\n",
      "2.239196300506592\n",
      "2.2391908168792725\n",
      "2.2391855716705322\n",
      "2.239180088043213\n",
      "2.2391743659973145\n",
      "2.2391693592071533\n",
      "2.239163875579834\n",
      "2.2391586303710938\n",
      "2.2391536235809326\n",
      "2.2391481399536133\n",
      "2.239143133163452\n",
      "2.239138126373291\n",
      "2.239132881164551\n",
      "2.2391276359558105\n",
      "2.2391228675842285\n",
      "2.2391180992126465\n",
      "2.2391128540039062\n",
      "2.239108085632324\n",
      "2.239103078842163\n",
      "2.239098310470581\n",
      "2.239093542098999\n",
      "2.239088773727417\n",
      "2.239084243774414\n",
      "2.239079475402832\n",
      "2.23907470703125\n",
      "2.239069938659668\n",
      "2.239065408706665\n",
      "2.239060878753662\n",
      "2.239056348800659\n",
      "2.2390518188476562\n",
      "2.2390475273132324\n",
      "2.2390429973602295\n",
      "2.2390384674072266\n",
      "2.239034414291382\n",
      "2.239029884338379\n",
      "2.239025592803955\n",
      "2.239021062850952\n",
      "2.2390170097351074\n",
      "2.2390129566192627\n",
      "2.2390084266662598\n",
      "2.239004135131836\n",
      "2.2390003204345703\n",
      "2.2389960289001465\n",
      "2.238992214202881\n",
      "2.238987922668457\n",
      "2.2389838695526123\n",
      "2.2389795780181885\n",
      "2.238975763320923\n",
      "2.2389719486236572\n",
      "2.2389678955078125\n",
      "2.238964080810547\n",
      "2.2389602661132812\n",
      "2.2389562129974365\n",
      "2.238952398300171\n",
      "2.238948345184326\n",
      "2.2389447689056396\n",
      "2.238940954208374\n",
      "2.2389373779296875\n",
      "2.238933563232422\n",
      "2.2389297485351562\n",
      "2.2389259338378906\n",
      "2.238922357559204\n",
      "2.2389190196990967\n",
      "2.23891544342041\n",
      "2.2389118671417236\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# gradient descent with indexing into W matrix\n",
    "for k in range(100):\n",
    "    \n",
    "    # forward pass\n",
    "    first_rows = W[xs[:, 0], :]\n",
    "    sec_rows = W[xs[:, 1] + 27, :]\n",
    "    logits = first_rows + sec_rows # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    loss = -probs[torch.arange(num), ys].log().mean() + 0.001*(W**2).mean()\n",
    "    print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    W.grad = None # set to zero the gradient\n",
    "    loss.backward()\n",
    "    \n",
    "    # update the weights\n",
    "    W.data += -50 * W.grad\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2386364936828613\n",
      "2.2386345863342285\n",
      "2.2386324405670166\n",
      "2.238630533218384\n",
      "2.238628387451172\n",
      "2.238626480102539\n",
      "2.238624095916748\n",
      "2.2386221885681152\n",
      "2.2386205196380615\n",
      "2.2386183738708496\n",
      "2.2386162281036377\n",
      "2.238614082336426\n",
      "2.238612413406372\n",
      "2.23861026763916\n",
      "2.2386083602905273\n",
      "2.2386064529418945\n",
      "2.2386043071746826\n",
      "2.238602638244629\n",
      "2.238600730895996\n",
      "2.238598585128784\n",
      "2.2385969161987305\n",
      "2.2385950088500977\n",
      "2.238593101501465\n",
      "2.238591194152832\n",
      "2.238589286804199\n",
      "2.2385873794555664\n",
      "2.2385852336883545\n",
      "2.23858380317688\n",
      "2.238581895828247\n",
      "2.2385799884796143\n",
      "2.2385783195495605\n",
      "2.2385761737823486\n",
      "2.238574743270874\n",
      "2.238572597503662\n",
      "2.2385706901550293\n",
      "2.2385692596435547\n",
      "2.2385671138763428\n",
      "2.238565683364868\n",
      "2.2385637760162354\n",
      "2.2385621070861816\n",
      "2.238560438156128\n",
      "2.238558530807495\n",
      "2.2385568618774414\n",
      "2.2385551929473877\n",
      "2.238553524017334\n",
      "2.2385518550872803\n",
      "2.2385501861572266\n",
      "2.2385482788085938\n",
      "2.23854660987854\n",
      "2.2385449409484863\n",
      "2.2385435104370117\n",
      "2.2385413646698\n",
      "2.238539934158325\n",
      "2.2385382652282715\n",
      "2.238536834716797\n",
      "2.238534688949585\n",
      "2.2385332584381104\n",
      "2.2385313510894775\n",
      "2.238530158996582\n",
      "2.238528251647949\n",
      "2.2385265827178955\n",
      "2.238524913787842\n",
      "2.238523483276367\n",
      "2.2385220527648926\n",
      "2.238520383834839\n",
      "2.2385189533233643\n",
      "2.2385172843933105\n",
      "2.238515615463257\n",
      "2.238513946533203\n",
      "2.2385125160217285\n",
      "2.238510847091675\n",
      "2.238509178161621\n",
      "2.2385077476501465\n",
      "2.238506317138672\n",
      "2.2385048866271973\n",
      "2.2385034561157227\n",
      "2.23850154876709\n",
      "2.2385001182556152\n",
      "2.2384989261627197\n",
      "2.238497257232666\n",
      "2.2384955883026123\n",
      "2.238494396209717\n",
      "2.238492727279663\n",
      "2.2384915351867676\n",
      "2.238489866256714\n",
      "2.2384884357452393\n",
      "2.2384867668151855\n",
      "2.23848557472229\n",
      "2.2384841442108154\n",
      "2.238482713699341\n",
      "2.238481044769287\n",
      "2.2384796142578125\n",
      "2.238478422164917\n",
      "2.2384769916534424\n",
      "2.2384755611419678\n",
      "2.238474130630493\n",
      "2.2384727001190186\n",
      "2.238471269607544\n",
      "2.2384698390960693\n",
      "2.2384681701660156\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# gradient descent with F.cross_entropy()\n",
    "for k in range(100):\n",
    "    \n",
    "    # forward pass\n",
    "    first_rows = W[xs[:, 0], :]\n",
    "    sec_rows = W[xs[:, 1] + 27, :]\n",
    "    logits = first_rows + sec_rows  # predict log-counts\n",
    "    probs = F.softmax(logits, dim=1)  # compute the probabilities\n",
    "    loss = F.cross_entropy(logits, ys) + 0.001*(W**2).mean()  # compute the loss\n",
    "    print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    W.grad = None  # set to zero the gradient\n",
    "    loss.backward()\n",
    "    \n",
    "    # update the weights\n",
    "    W.data += -50 * W.grad\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-196-ec6f13f8ce36>, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-196-ec6f13f8ce36>\"\u001b[1;36m, line \u001b[1;32m21\u001b[0m\n\u001b[1;33m    print('done')''\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# OG gradient descent\n",
    "for k in range(100):\n",
    "    \n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "    # print(xenc.shape)\n",
    "    xenc = xenc.reshape(-1,54)\n",
    "    # print('xenc[0] after reshaping', xenc[0])\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    loss = -probs[torch.arange(num), ys].log().mean() + 0.001*(W**2).mean()\n",
    "    print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    W.grad = None # set to zero the gradient\n",
    "    loss.backward()\n",
    "    \n",
    "    # update the weights\n",
    "    W.data += -50 * W.grad\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0297, -0.8263, -0.1698,  0.6717,  0.0764],\n",
      "        [-0.3290,  0.1966, -0.1284,  0.4243, -0.6773],\n",
      "        [ 0.8037,  1.8986,  1.4281, -0.0186, -0.9951]], requires_grad=True)\n",
      "target\n",
      "tensor([3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# Example of target with class indices\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "print(input)\n",
    "print('target')\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2832,  0.5422,  0.9054,  0.3011,  0.3526],\n",
      "        [-0.4428,  0.9215, -0.3576,  0.9708, -0.2848],\n",
      "        [ 0.4708, -0.4528, -0.3784, -0.5295,  1.0000]], requires_grad=True)\n",
      "target\n",
      "tensor([[0.4084, 0.1156, 0.0268, 0.0547, 0.3945],\n",
      "        [0.0235, 0.0725, 0.2474, 0.0662, 0.5904],\n",
      "        [0.0386, 0.3852, 0.0624, 0.1963, 0.3175]])\n"
     ]
    }
   ],
   "source": [
    "# Example of target with class probabilities\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "print(input)\n",
    "print('target')\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of target with class indices\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "\n",
    "# Example of target with class probabilities\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ron.\n",
      "urde.\n",
      "byn.\n",
      "huril.\n",
      "wiv.\n",
      "rachadek.\n",
      "orbel.\n",
      "zareele.\n",
      "rwasharriyanilen.\n",
      "tent.\n",
      "ahiakhiesa.\n",
      "aidosaidgr.\n",
      "avira.\n",
      "llymiliyny.\n",
      "allid.\n",
      "harius.\n",
      "uzeid.\n",
      "aanabrlyn.\n",
      "urish.\n",
      "emralikayn.\n",
      "atirawh.\n",
      "ia.\n",
      "abrie.\n",
      "omah.\n",
      "rniaghanlyilean.\n",
      "ursi.\n",
      "abyah.\n",
      "azzyn.\n",
      "iu.\n",
      "on.\n",
      "reeris.\n",
      "iz.\n",
      "poptielon.\n",
      "hon.\n",
      "hot.\n",
      "aalya.\n",
      "ubrelinaynsen.\n",
      "adiny.\n",
      "nisharix.\n",
      "arde.\n",
      "ana.\n",
      "adarlereclenniagh.\n",
      "yl.\n",
      "fry.\n",
      "orilya.\n",
      "zo.\n",
      "arounnasame.\n",
      "ah.\n",
      "udejontegs.\n",
      "urandoryanyn.\n",
      "wa.\n",
      "uva.\n",
      "rllyanaseesmar.\n",
      "ah.\n",
      "aunicamianne.\n",
      "harial.\n",
      "rekhommairahyri.\n",
      "quia.\n",
      "on.\n",
      "orie.\n",
      "hanariekhylikhola.\n",
      "ie.\n",
      "ryanahidin.\n",
      "maem.\n",
      "ncliashan.\n",
      "hesyndair.\n",
      "cel.\n",
      "vane.\n",
      "ockola.\n",
      "chayllyah.\n",
      "zabdeldeelario.\n",
      "ran.\n",
      "avangeleyrasslaydimeelaaen.\n",
      "yia.\n",
      "la.\n",
      "ay.\n",
      "allahlian.\n",
      "righelephon.\n",
      "avina.\n",
      "uly.\n",
      "ferasimawan.\n",
      "nillisio.\n",
      "le.\n",
      "ya.\n",
      "ua.\n",
      "ranahmirh.\n",
      "rahmar.\n",
      "ataanie.\n",
      "anna.\n",
      "yah.\n",
      "ar.\n",
      "ust.\n",
      "yone.\n",
      "audamomarcienef.\n",
      "ro.\n",
      "hava.\n",
      "hal.\n",
      "ave.\n",
      "gelya.\n",
      "eameigha.\n"
     ]
    }
   ],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2247403657)\n",
    "\n",
    "out = []\n",
    "ix = 0\n",
    "iy = 0\n",
    "# xenc = F.one_hot(torch.tensor([ix, iy]), num_classes=27).float() # TODO: I need to input two values to trigram model!\n",
    "# xenc = xenc.reshape(-1, 54)\n",
    "# xenc\n",
    "    \n",
    "for i in range(100):\n",
    "    \n",
    "    out = []\n",
    "    ix = 0\n",
    "    iy = 0\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([ix, iy]), num_classes=27).float()\n",
    "        xenc = xenc.reshape(-1,54) # (num_of_examples, vocab_size*2)\n",
    "        logits = xenc @ W # predict log-counts\n",
    "        counts = logits.exp() # counts, equivalent to N\n",
    "        p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "        # print(p.shape)\n",
    "        # ------------\n",
    "        \n",
    "        ix = iy\n",
    "        iy = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[iy])\n",
    "        if iy == 0: \n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  19296\n"
     ]
    }
   ],
   "source": [
    "# create the dataset for devSet\n",
    "dXs, dYs = [],[]\n",
    "for w in devSet:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        dXs.append((ix1,ix2))\n",
    "        dYs.append(ix3)\n",
    "\n",
    "dXs = torch.tensor(dXs)\n",
    "dYs = torch.tensor(dYs)\n",
    "num = int(dXs.nelement()/2)\n",
    "print('number of examples: ', num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.095712423324585\n",
      "2.066075563430786\n",
      "2.055318593978882\n",
      "2.048307180404663\n",
      "2.0439634323120117\n",
      "2.0400307178497314\n",
      "2.0375730991363525\n",
      "2.0348267555236816\n",
      "2.033247947692871\n",
      "2.0311121940612793\n",
      "2.0300114154815674\n",
      "2.028247833251953\n",
      "2.0274317264556885\n",
      "2.0259251594543457\n",
      "2.0252914428710938\n",
      "2.0239806175231934\n",
      "2.023470878601074\n",
      "2.0223186016082764\n",
      "2.0218985080718994\n",
      "2.020878314971924\n",
      "2.0205256938934326\n",
      "2.0196173191070557\n",
      "2.019317388534546\n",
      "2.018505334854126\n",
      "2.0182478427886963\n",
      "2.0175182819366455\n",
      "2.0172955989837646\n",
      "2.0166380405426025\n",
      "2.0164434909820557\n",
      "2.015847682952881\n",
      "2.0156776905059814\n",
      "2.0151355266571045\n",
      "2.014986515045166\n",
      "2.014491081237793\n",
      "2.014359712600708\n",
      "2.0139060020446777\n",
      "2.013789653778076\n",
      "2.0133721828460693\n",
      "2.0132691860198975\n",
      "2.0128839015960693\n",
      "2.0127923488616943\n",
      "2.0124351978302\n",
      "2.0123541355133057\n",
      "2.0120222568511963\n",
      "2.0119497776031494\n",
      "2.0116405487060547\n",
      "2.011575937271118\n",
      "2.0112874507904053\n",
      "2.0112297534942627\n",
      "2.0109593868255615\n",
      "2.0109078884124756\n",
      "2.010653257369995\n",
      "2.010607957839966\n",
      "2.0103683471679688\n",
      "2.0103280544281006\n",
      "2.010101795196533\n",
      "2.010065793991089\n",
      "2.009852170944214\n",
      "2.0098202228546143\n",
      "2.009617328643799\n",
      "2.009589195251465\n",
      "2.00939679145813\n",
      "2.0093722343444824\n",
      "2.0091888904571533\n",
      "2.009167432785034\n",
      "2.0089926719665527\n",
      "2.008974075317383\n",
      "2.0088071823120117\n",
      "2.008790969848633\n",
      "2.008631706237793\n",
      "2.008617639541626\n",
      "2.008464813232422\n",
      "2.008453369140625\n",
      "2.0083067417144775\n",
      "2.0082974433898926\n",
      "2.0081565380096436\n",
      "2.008148670196533\n",
      "2.0080134868621826\n",
      "2.008007049560547\n",
      "2.0078771114349365\n",
      "2.0078723430633545\n",
      "2.007747173309326\n",
      "2.0077438354492188\n",
      "2.007622718811035\n",
      "2.0076208114624023\n",
      "2.0075037479400635\n",
      "2.007503032684326\n",
      "2.007390022277832\n",
      "2.0073904991149902\n",
      "2.0072810649871826\n",
      "2.0072824954986572\n",
      "2.007176637649536\n",
      "2.007178783416748\n",
      "2.0070762634277344\n",
      "2.007079601287842\n",
      "2.0069799423217773\n",
      "2.006983995437622\n",
      "2.0068869590759277\n",
      "2.006891965866089\n",
      "2.0067975521087646\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(100):\n",
    "    \n",
    "    # forward pass\n",
    "    xenc = F.one_hot(dXs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "    xenc = xenc.reshape(-1,54)\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    loss = -probs[torch.arange(num), dYs].log().mean() + 0.00019*(W**2).mean()\n",
    "    print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    W.grad = None # set to zero the gradient\n",
    "    loss.backward()\n",
    "    \n",
    "    # update the weights\n",
    "    W.data += -50 * W.grad\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(-56947.0664, grad_fn=<AddBackward0>)\n",
      "nll=tensor(56947.0664, grad_fn=<NegBackward0>)\n",
      "3.538623332977295\n"
     ]
    }
   ],
   "source": [
    "# Evaluate against dev set\n",
    "\n",
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "for w in devSet:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(  chs ,  chs[1:], chs[3:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        trigramPre = torch.tensor((ix1,ix2))\n",
    "        xenc = F.one_hot(trigramPre, num_classes=27).float() # input to the network: one-hot encoding\n",
    "        xenc = xenc.reshape(-1,54)\n",
    "        logits = xenc @ W # predict log-counts\n",
    "        counts = logits.exp() # counts, equivalent to N\n",
    "        probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "        #print(probs.shape)\n",
    "#         print(probs[0,ix3])\n",
    "        prob = probs[0,ix3]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "#         print(f'{ch1}{ch2}{ch3}: {prob:.4f} {logprob:.4f}')\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')\n",
    "print(f'{nll/n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(-64119.7266, grad_fn=<AddBackward0>)\n",
      "nll=tensor(64119.7266, grad_fn=<NegBackward0>)\n",
      "3.927220344543457\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "for w in testSet:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(  chs ,  chs[1:], chs[3:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        trigramPre = torch.tensor((ix1,ix2))\n",
    "        xenc = F.one_hot(trigramPre, num_classes=27).float() # input to the network: one-hot encoding\n",
    "        xenc = xenc.reshape(-1,54)\n",
    "        logits = xenc @ W # predict log-counts\n",
    "        counts = logits.exp() # counts, equivalent to N\n",
    "        probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "        #print(probs.shape)\n",
    "#         print(probs[0,ix3])\n",
    "        prob = probs[0,ix3]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "#         print(f'{ch1}{ch2}{ch3}: {prob:.4f} {logprob:.4f}')\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')\n",
    "print(f'{nll/n}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
