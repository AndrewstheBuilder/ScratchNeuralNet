{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "X, Y = [], []\n",
    "for w in words:\n",
    "    \n",
    "#     print(w)\n",
    "    context = [0]*block_size\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "#         print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        context = context[1:] + [ix] # crop and append\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.int64, torch.Size([228146]), torch.int64)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((27,2)) # each one of 27 char will have 2 dimensional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 3, 2])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X].shape # for every one of the 32 x 3 characters we got a row of 2 from the C matrix C[X] != C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = C[X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the hidden layer\n",
    "W1 = torch.randn((6, 100)) #(num_inputs = 3*2, variable number of neurons)\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-355728df8662>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0memb\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mW1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)"
     ]
    }
   ],
   "source": [
    "emb @ W1 + b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01]],\n",
       "\n",
       "        [[-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-6.4305e-01,  9.2540e-01]],\n",
       "\n",
       "        [[-9.1079e-02, -1.2058e-01],\n",
       "         [-6.4305e-01,  9.2540e-01],\n",
       "         [-2.5104e-01,  1.5556e+00]],\n",
       "\n",
       "        [[-6.4305e-01,  9.2540e-01],\n",
       "         [-2.5104e-01,  1.5556e+00],\n",
       "         [-2.5104e-01,  1.5556e+00]],\n",
       "\n",
       "        [[-2.5104e-01,  1.5556e+00],\n",
       "         [-2.5104e-01,  1.5556e+00],\n",
       "         [-1.1086e+00, -2.3352e+00]],\n",
       "\n",
       "        [[-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01]],\n",
       "\n",
       "        [[-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [ 2.3154e+00,  5.1048e-01]],\n",
       "\n",
       "        [[-9.1079e-02, -1.2058e-01],\n",
       "         [ 2.3154e+00,  5.1048e-01],\n",
       "         [-1.2148e+00, -1.1060e-01]],\n",
       "\n",
       "        [[ 2.3154e+00,  5.1048e-01],\n",
       "         [-1.2148e+00, -1.1060e-01],\n",
       "         [-2.1194e-01,  6.2278e-01]],\n",
       "\n",
       "        [[-1.2148e+00, -1.1060e-01],\n",
       "         [-2.1194e-01,  6.2278e-01],\n",
       "         [-1.6346e-01,  1.3759e+00]],\n",
       "\n",
       "        [[-2.1194e-01,  6.2278e-01],\n",
       "         [-1.6346e-01,  1.3759e+00],\n",
       "         [-2.1194e-01,  6.2278e-01]],\n",
       "\n",
       "        [[-1.6346e-01,  1.3759e+00],\n",
       "         [-2.1194e-01,  6.2278e-01],\n",
       "         [-1.1086e+00, -2.3352e+00]],\n",
       "\n",
       "        [[-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01]],\n",
       "\n",
       "        [[-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-1.1086e+00, -2.3352e+00]],\n",
       "\n",
       "        [[-9.1079e-02, -1.2058e-01],\n",
       "         [-1.1086e+00, -2.3352e+00],\n",
       "         [-1.6346e-01,  1.3759e+00]],\n",
       "\n",
       "        [[-1.1086e+00, -2.3352e+00],\n",
       "         [-1.6346e-01,  1.3759e+00],\n",
       "         [-1.1086e+00, -2.3352e+00]],\n",
       "\n",
       "        [[-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01]],\n",
       "\n",
       "        [[-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-2.1194e-01,  6.2278e-01]],\n",
       "\n",
       "        [[-9.1079e-02, -1.2058e-01],\n",
       "         [-2.1194e-01,  6.2278e-01],\n",
       "         [ 5.5838e-01,  1.7255e-01]],\n",
       "\n",
       "        [[-2.1194e-01,  6.2278e-01],\n",
       "         [ 5.5838e-01,  1.7255e-01],\n",
       "         [-1.1086e+00, -2.3352e+00]],\n",
       "\n",
       "        [[ 5.5838e-01,  1.7255e-01],\n",
       "         [-1.1086e+00, -2.3352e+00],\n",
       "         [ 8.4964e-01, -2.1192e-03]],\n",
       "\n",
       "        [[-1.1086e+00, -2.3352e+00],\n",
       "         [ 8.4964e-01, -2.1192e-03],\n",
       "         [-6.4305e-01,  9.2540e-01]],\n",
       "\n",
       "        [[ 8.4964e-01, -2.1192e-03],\n",
       "         [-6.4305e-01,  9.2540e-01],\n",
       "         [-1.2148e+00, -1.1060e-01]],\n",
       "\n",
       "        [[-6.4305e-01,  9.2540e-01],\n",
       "         [-1.2148e+00, -1.1060e-01],\n",
       "         [-1.2148e+00, -1.1060e-01]],\n",
       "\n",
       "        [[-1.2148e+00, -1.1060e-01],\n",
       "         [-1.2148e+00, -1.1060e-01],\n",
       "         [-1.1086e+00, -2.3352e+00]],\n",
       "\n",
       "        [[-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01]],\n",
       "\n",
       "        [[-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [ 5.5838e-01,  1.7255e-01]],\n",
       "\n",
       "        [[-9.1079e-02, -1.2058e-01],\n",
       "         [ 5.5838e-01,  1.7255e-01],\n",
       "         [ 2.3154e+00,  5.1048e-01]],\n",
       "\n",
       "        [[ 5.5838e-01,  1.7255e-01],\n",
       "         [ 2.3154e+00,  5.1048e-01],\n",
       "         [ 7.0854e-01, -9.1979e-01]],\n",
       "\n",
       "        [[ 2.3154e+00,  5.1048e-01],\n",
       "         [ 7.0854e-01, -9.1979e-01],\n",
       "         [-1.7234e+00,  6.2434e-02]],\n",
       "\n",
       "        [[ 7.0854e-01, -9.1979e-01],\n",
       "         [-1.7234e+00,  6.2434e-02],\n",
       "         [-2.1194e-01,  6.2278e-01]],\n",
       "\n",
       "        [[-1.7234e+00,  6.2434e-02],\n",
       "         [-2.1194e-01,  6.2278e-01],\n",
       "         [-1.1086e+00, -2.3352e+00]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C[0] tensor([-0.0911, -0.1206])\n",
      "X[0] tensor([0, 0, 0])\n",
      "emb[0][0] tensor([-0.0911, -0.1206])\n",
      "emb[0][1] tensor([-0.0911, -0.1206])\n",
      "emb[0][2] tensor([-0.0911, -0.1206])\n",
      "-----------------------------------------------------------\n",
      "C[5] tensor([-0.6430,  0.9254])\n",
      "X[1] tensor([0, 0, 5])\n",
      "emb[1] tensor([[-0.0911, -0.1206],\n",
      "        [-0.0911, -0.1206],\n",
      "        [-0.6430,  0.9254]])\n"
     ]
    }
   ],
   "source": [
    "print('C[0]', C[0])\n",
    "print('X[0]', X[0])\n",
    "print('emb[0][0]', emb[0][0])\n",
    "print('emb[0][1]', emb[0][1])\n",
    "print('emb[0][2]', emb[0][2])\n",
    "print('-----------------------------------------------------------')\n",
    "print('C[5]',C[5])\n",
    "print('X[1]', X[1])\n",
    "print('emb[1]',emb[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.1079e-02, -1.2058e-01],\n",
       "        [-9.1079e-02, -1.2058e-01],\n",
       "        [-9.1079e-02, -1.2058e-01],\n",
       "        [-6.4305e-01,  9.2540e-01],\n",
       "        [-2.5104e-01,  1.5556e+00],\n",
       "        [-9.1079e-02, -1.2058e-01],\n",
       "        [-9.1079e-02, -1.2058e-01],\n",
       "        [-9.1079e-02, -1.2058e-01],\n",
       "        [ 2.3154e+00,  5.1048e-01],\n",
       "        [-1.2148e+00, -1.1060e-01],\n",
       "        [-2.1194e-01,  6.2278e-01],\n",
       "        [-1.6346e-01,  1.3759e+00],\n",
       "        [-9.1079e-02, -1.2058e-01],\n",
       "        [-9.1079e-02, -1.2058e-01],\n",
       "        [-9.1079e-02, -1.2058e-01],\n",
       "        [-1.1086e+00, -2.3352e+00],\n",
       "        [-9.1079e-02, -1.2058e-01],\n",
       "        [-9.1079e-02, -1.2058e-01],\n",
       "        [-9.1079e-02, -1.2058e-01],\n",
       "        [-2.1194e-01,  6.2278e-01],\n",
       "        [ 5.5838e-01,  1.7255e-01],\n",
       "        [-1.1086e+00, -2.3352e+00],\n",
       "        [ 8.4964e-01, -2.1192e-03],\n",
       "        [-6.4305e-01,  9.2540e-01],\n",
       "        [-1.2148e+00, -1.1060e-01],\n",
       "        [-9.1079e-02, -1.2058e-01],\n",
       "        [-9.1079e-02, -1.2058e-01],\n",
       "        [-9.1079e-02, -1.2058e-01],\n",
       "        [ 5.5838e-01,  1.7255e-01],\n",
       "        [ 2.3154e+00,  5.1048e-01],\n",
       "        [ 7.0854e-01, -9.1979e-01],\n",
       "        [-1.7234e+00,  6.2434e-02]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([emb[:, 0, :],emb[:, 1, :],emb[:, 2, :]], 1).shape # [embeddings for all of the first characters, second chars, third chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-6.4305e-01,  9.2540e-01],\n",
       "         [-2.5104e-01,  1.5556e+00],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [ 2.3154e+00,  5.1048e-01],\n",
       "         [-1.2148e+00, -1.1060e-01],\n",
       "         [-2.1194e-01,  6.2278e-01],\n",
       "         [-1.6346e-01,  1.3759e+00],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-1.1086e+00, -2.3352e+00],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-2.1194e-01,  6.2278e-01],\n",
       "         [ 5.5838e-01,  1.7255e-01],\n",
       "         [-1.1086e+00, -2.3352e+00],\n",
       "         [ 8.4964e-01, -2.1192e-03],\n",
       "         [-6.4305e-01,  9.2540e-01],\n",
       "         [-1.2148e+00, -1.1060e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [ 5.5838e-01,  1.7255e-01],\n",
       "         [ 2.3154e+00,  5.1048e-01],\n",
       "         [ 7.0854e-01, -9.1979e-01],\n",
       "         [-1.7234e+00,  6.2434e-02]]),\n",
       " tensor([[-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-6.4305e-01,  9.2540e-01],\n",
       "         [-2.5104e-01,  1.5556e+00],\n",
       "         [-2.5104e-01,  1.5556e+00],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [ 2.3154e+00,  5.1048e-01],\n",
       "         [-1.2148e+00, -1.1060e-01],\n",
       "         [-2.1194e-01,  6.2278e-01],\n",
       "         [-1.6346e-01,  1.3759e+00],\n",
       "         [-2.1194e-01,  6.2278e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-1.1086e+00, -2.3352e+00],\n",
       "         [-1.6346e-01,  1.3759e+00],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-2.1194e-01,  6.2278e-01],\n",
       "         [ 5.5838e-01,  1.7255e-01],\n",
       "         [-1.1086e+00, -2.3352e+00],\n",
       "         [ 8.4964e-01, -2.1192e-03],\n",
       "         [-6.4305e-01,  9.2540e-01],\n",
       "         [-1.2148e+00, -1.1060e-01],\n",
       "         [-1.2148e+00, -1.1060e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [ 5.5838e-01,  1.7255e-01],\n",
       "         [ 2.3154e+00,  5.1048e-01],\n",
       "         [ 7.0854e-01, -9.1979e-01],\n",
       "         [-1.7234e+00,  6.2434e-02],\n",
       "         [-2.1194e-01,  6.2278e-01]]),\n",
       " tensor([[-9.1079e-02, -1.2058e-01],\n",
       "         [-6.4305e-01,  9.2540e-01],\n",
       "         [-2.5104e-01,  1.5556e+00],\n",
       "         [-2.5104e-01,  1.5556e+00],\n",
       "         [-1.1086e+00, -2.3352e+00],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [ 2.3154e+00,  5.1048e-01],\n",
       "         [-1.2148e+00, -1.1060e-01],\n",
       "         [-2.1194e-01,  6.2278e-01],\n",
       "         [-1.6346e-01,  1.3759e+00],\n",
       "         [-2.1194e-01,  6.2278e-01],\n",
       "         [-1.1086e+00, -2.3352e+00],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-1.1086e+00, -2.3352e+00],\n",
       "         [-1.6346e-01,  1.3759e+00],\n",
       "         [-1.1086e+00, -2.3352e+00],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-2.1194e-01,  6.2278e-01],\n",
       "         [ 5.5838e-01,  1.7255e-01],\n",
       "         [-1.1086e+00, -2.3352e+00],\n",
       "         [ 8.4964e-01, -2.1192e-03],\n",
       "         [-6.4305e-01,  9.2540e-01],\n",
       "         [-1.2148e+00, -1.1060e-01],\n",
       "         [-1.2148e+00, -1.1060e-01],\n",
       "         [-1.1086e+00, -2.3352e+00],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [ 5.5838e-01,  1.7255e-01],\n",
       "         [ 2.3154e+00,  5.1048e-01],\n",
       "         [ 7.0854e-01, -9.1979e-01],\n",
       "         [-1.7234e+00,  6.2434e-02],\n",
       "         [-2.1194e-01,  6.2278e-01],\n",
       "         [-1.1086e+00, -2.3352e+00]]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unbind(emb,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.1079e-02, -1.2058e-01, -9.1079e-02, -1.2058e-01, -9.1079e-02,\n",
       "         -1.2058e-01],\n",
       "        [-9.1079e-02, -1.2058e-01, -9.1079e-02, -1.2058e-01, -6.4305e-01,\n",
       "          9.2540e-01],\n",
       "        [-9.1079e-02, -1.2058e-01, -6.4305e-01,  9.2540e-01, -2.5104e-01,\n",
       "          1.5556e+00],\n",
       "        [-6.4305e-01,  9.2540e-01, -2.5104e-01,  1.5556e+00, -2.5104e-01,\n",
       "          1.5556e+00],\n",
       "        [-2.5104e-01,  1.5556e+00, -2.5104e-01,  1.5556e+00, -1.1086e+00,\n",
       "         -2.3352e+00],\n",
       "        [-9.1079e-02, -1.2058e-01, -9.1079e-02, -1.2058e-01, -9.1079e-02,\n",
       "         -1.2058e-01],\n",
       "        [-9.1079e-02, -1.2058e-01, -9.1079e-02, -1.2058e-01,  2.3154e+00,\n",
       "          5.1048e-01],\n",
       "        [-9.1079e-02, -1.2058e-01,  2.3154e+00,  5.1048e-01, -1.2148e+00,\n",
       "         -1.1060e-01],\n",
       "        [ 2.3154e+00,  5.1048e-01, -1.2148e+00, -1.1060e-01, -2.1194e-01,\n",
       "          6.2278e-01],\n",
       "        [-1.2148e+00, -1.1060e-01, -2.1194e-01,  6.2278e-01, -1.6346e-01,\n",
       "          1.3759e+00],\n",
       "        [-2.1194e-01,  6.2278e-01, -1.6346e-01,  1.3759e+00, -2.1194e-01,\n",
       "          6.2278e-01],\n",
       "        [-1.6346e-01,  1.3759e+00, -2.1194e-01,  6.2278e-01, -1.1086e+00,\n",
       "         -2.3352e+00],\n",
       "        [-9.1079e-02, -1.2058e-01, -9.1079e-02, -1.2058e-01, -9.1079e-02,\n",
       "         -1.2058e-01],\n",
       "        [-9.1079e-02, -1.2058e-01, -9.1079e-02, -1.2058e-01, -1.1086e+00,\n",
       "         -2.3352e+00],\n",
       "        [-9.1079e-02, -1.2058e-01, -1.1086e+00, -2.3352e+00, -1.6346e-01,\n",
       "          1.3759e+00],\n",
       "        [-1.1086e+00, -2.3352e+00, -1.6346e-01,  1.3759e+00, -1.1086e+00,\n",
       "         -2.3352e+00],\n",
       "        [-9.1079e-02, -1.2058e-01, -9.1079e-02, -1.2058e-01, -9.1079e-02,\n",
       "         -1.2058e-01],\n",
       "        [-9.1079e-02, -1.2058e-01, -9.1079e-02, -1.2058e-01, -2.1194e-01,\n",
       "          6.2278e-01],\n",
       "        [-9.1079e-02, -1.2058e-01, -2.1194e-01,  6.2278e-01,  5.5838e-01,\n",
       "          1.7255e-01],\n",
       "        [-2.1194e-01,  6.2278e-01,  5.5838e-01,  1.7255e-01, -1.1086e+00,\n",
       "         -2.3352e+00],\n",
       "        [ 5.5838e-01,  1.7255e-01, -1.1086e+00, -2.3352e+00,  8.4964e-01,\n",
       "         -2.1192e-03],\n",
       "        [-1.1086e+00, -2.3352e+00,  8.4964e-01, -2.1192e-03, -6.4305e-01,\n",
       "          9.2540e-01],\n",
       "        [ 8.4964e-01, -2.1192e-03, -6.4305e-01,  9.2540e-01, -1.2148e+00,\n",
       "         -1.1060e-01],\n",
       "        [-6.4305e-01,  9.2540e-01, -1.2148e+00, -1.1060e-01, -1.2148e+00,\n",
       "         -1.1060e-01],\n",
       "        [-1.2148e+00, -1.1060e-01, -1.2148e+00, -1.1060e-01, -1.1086e+00,\n",
       "         -2.3352e+00],\n",
       "        [-9.1079e-02, -1.2058e-01, -9.1079e-02, -1.2058e-01, -9.1079e-02,\n",
       "         -1.2058e-01],\n",
       "        [-9.1079e-02, -1.2058e-01, -9.1079e-02, -1.2058e-01,  5.5838e-01,\n",
       "          1.7255e-01],\n",
       "        [-9.1079e-02, -1.2058e-01,  5.5838e-01,  1.7255e-01,  2.3154e+00,\n",
       "          5.1048e-01],\n",
       "        [ 5.5838e-01,  1.7255e-01,  2.3154e+00,  5.1048e-01,  7.0854e-01,\n",
       "         -9.1979e-01],\n",
       "        [ 2.3154e+00,  5.1048e-01,  7.0854e-01, -9.1979e-01, -1.7234e+00,\n",
       "          6.2434e-02],\n",
       "        [ 7.0854e-01, -9.1979e-01, -1.7234e+00,  6.2434e-02, -2.1194e-01,\n",
       "          6.2278e-01],\n",
       "        [-1.7234e+00,  6.2434e-02, -2.1194e-01,  6.2278e-01, -1.1086e+00,\n",
       "         -2.3352e+00]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(torch.unbind(emb,1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 2,  3],\n",
       "         [ 4,  5]],\n",
       "\n",
       "        [[ 6,  7],\n",
       "         [ 8,  9],\n",
       "         [10, 11]],\n",
       "\n",
       "        [[12, 13],\n",
       "         [14, 15],\n",
       "         [16, 17]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(3,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01]],\n",
       "\n",
       "        [[-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-6.4305e-01,  9.2540e-01]],\n",
       "\n",
       "        [[-9.1079e-02, -1.2058e-01],\n",
       "         [-6.4305e-01,  9.2540e-01],\n",
       "         [-2.5104e-01,  1.5556e+00]],\n",
       "\n",
       "        [[-6.4305e-01,  9.2540e-01],\n",
       "         [-2.5104e-01,  1.5556e+00],\n",
       "         [-2.5104e-01,  1.5556e+00]],\n",
       "\n",
       "        [[-2.5104e-01,  1.5556e+00],\n",
       "         [-2.5104e-01,  1.5556e+00],\n",
       "         [-1.1086e+00, -2.3352e+00]],\n",
       "\n",
       "        [[-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01]],\n",
       "\n",
       "        [[-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [ 2.3154e+00,  5.1048e-01]],\n",
       "\n",
       "        [[-9.1079e-02, -1.2058e-01],\n",
       "         [ 2.3154e+00,  5.1048e-01],\n",
       "         [-1.2148e+00, -1.1060e-01]],\n",
       "\n",
       "        [[ 2.3154e+00,  5.1048e-01],\n",
       "         [-1.2148e+00, -1.1060e-01],\n",
       "         [-2.1194e-01,  6.2278e-01]],\n",
       "\n",
       "        [[-1.2148e+00, -1.1060e-01],\n",
       "         [-2.1194e-01,  6.2278e-01],\n",
       "         [-1.6346e-01,  1.3759e+00]],\n",
       "\n",
       "        [[-2.1194e-01,  6.2278e-01],\n",
       "         [-1.6346e-01,  1.3759e+00],\n",
       "         [-2.1194e-01,  6.2278e-01]],\n",
       "\n",
       "        [[-1.6346e-01,  1.3759e+00],\n",
       "         [-2.1194e-01,  6.2278e-01],\n",
       "         [-1.1086e+00, -2.3352e+00]],\n",
       "\n",
       "        [[-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01]],\n",
       "\n",
       "        [[-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-1.1086e+00, -2.3352e+00]],\n",
       "\n",
       "        [[-9.1079e-02, -1.2058e-01],\n",
       "         [-1.1086e+00, -2.3352e+00],\n",
       "         [-1.6346e-01,  1.3759e+00]],\n",
       "\n",
       "        [[-1.1086e+00, -2.3352e+00],\n",
       "         [-1.6346e-01,  1.3759e+00],\n",
       "         [-1.1086e+00, -2.3352e+00]],\n",
       "\n",
       "        [[-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01]],\n",
       "\n",
       "        [[-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-2.1194e-01,  6.2278e-01]],\n",
       "\n",
       "        [[-9.1079e-02, -1.2058e-01],\n",
       "         [-2.1194e-01,  6.2278e-01],\n",
       "         [ 5.5838e-01,  1.7255e-01]],\n",
       "\n",
       "        [[-2.1194e-01,  6.2278e-01],\n",
       "         [ 5.5838e-01,  1.7255e-01],\n",
       "         [-1.1086e+00, -2.3352e+00]],\n",
       "\n",
       "        [[ 5.5838e-01,  1.7255e-01],\n",
       "         [-1.1086e+00, -2.3352e+00],\n",
       "         [ 8.4964e-01, -2.1192e-03]],\n",
       "\n",
       "        [[-1.1086e+00, -2.3352e+00],\n",
       "         [ 8.4964e-01, -2.1192e-03],\n",
       "         [-6.4305e-01,  9.2540e-01]],\n",
       "\n",
       "        [[ 8.4964e-01, -2.1192e-03],\n",
       "         [-6.4305e-01,  9.2540e-01],\n",
       "         [-1.2148e+00, -1.1060e-01]],\n",
       "\n",
       "        [[-6.4305e-01,  9.2540e-01],\n",
       "         [-1.2148e+00, -1.1060e-01],\n",
       "         [-1.2148e+00, -1.1060e-01]],\n",
       "\n",
       "        [[-1.2148e+00, -1.1060e-01],\n",
       "         [-1.2148e+00, -1.1060e-01],\n",
       "         [-1.1086e+00, -2.3352e+00]],\n",
       "\n",
       "        [[-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01]],\n",
       "\n",
       "        [[-9.1079e-02, -1.2058e-01],\n",
       "         [-9.1079e-02, -1.2058e-01],\n",
       "         [ 5.5838e-01,  1.7255e-01]],\n",
       "\n",
       "        [[-9.1079e-02, -1.2058e-01],\n",
       "         [ 5.5838e-01,  1.7255e-01],\n",
       "         [ 2.3154e+00,  5.1048e-01]],\n",
       "\n",
       "        [[ 5.5838e-01,  1.7255e-01],\n",
       "         [ 2.3154e+00,  5.1048e-01],\n",
       "         [ 7.0854e-01, -9.1979e-01]],\n",
       "\n",
       "        [[ 2.3154e+00,  5.1048e-01],\n",
       "         [ 7.0854e-01, -9.1979e-01],\n",
       "         [-1.7234e+00,  6.2434e-02]],\n",
       "\n",
       "        [[ 7.0854e-01, -9.1979e-01],\n",
       "         [-1.7234e+00,  6.2434e-02],\n",
       "         [-2.1194e-01,  6.2278e-01]],\n",
       "\n",
       "        [[-1.7234e+00,  6.2434e-02],\n",
       "         [-2.1194e-01,  6.2278e-01],\n",
       "         [-1.1086e+00, -2.3352e+00]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.1079e-02, -1.2058e-01, -9.1079e-02, -1.2058e-01, -9.1079e-02,\n",
       "         -1.2058e-01],\n",
       "        [-9.1079e-02, -1.2058e-01, -9.1079e-02, -1.2058e-01, -6.4305e-01,\n",
       "          9.2540e-01],\n",
       "        [-9.1079e-02, -1.2058e-01, -6.4305e-01,  9.2540e-01, -2.5104e-01,\n",
       "          1.5556e+00],\n",
       "        [-6.4305e-01,  9.2540e-01, -2.5104e-01,  1.5556e+00, -2.5104e-01,\n",
       "          1.5556e+00],\n",
       "        [-2.5104e-01,  1.5556e+00, -2.5104e-01,  1.5556e+00, -1.1086e+00,\n",
       "         -2.3352e+00],\n",
       "        [-9.1079e-02, -1.2058e-01, -9.1079e-02, -1.2058e-01, -9.1079e-02,\n",
       "         -1.2058e-01],\n",
       "        [-9.1079e-02, -1.2058e-01, -9.1079e-02, -1.2058e-01,  2.3154e+00,\n",
       "          5.1048e-01],\n",
       "        [-9.1079e-02, -1.2058e-01,  2.3154e+00,  5.1048e-01, -1.2148e+00,\n",
       "         -1.1060e-01],\n",
       "        [ 2.3154e+00,  5.1048e-01, -1.2148e+00, -1.1060e-01, -2.1194e-01,\n",
       "          6.2278e-01],\n",
       "        [-1.2148e+00, -1.1060e-01, -2.1194e-01,  6.2278e-01, -1.6346e-01,\n",
       "          1.3759e+00],\n",
       "        [-2.1194e-01,  6.2278e-01, -1.6346e-01,  1.3759e+00, -2.1194e-01,\n",
       "          6.2278e-01],\n",
       "        [-1.6346e-01,  1.3759e+00, -2.1194e-01,  6.2278e-01, -1.1086e+00,\n",
       "         -2.3352e+00],\n",
       "        [-9.1079e-02, -1.2058e-01, -9.1079e-02, -1.2058e-01, -9.1079e-02,\n",
       "         -1.2058e-01],\n",
       "        [-9.1079e-02, -1.2058e-01, -9.1079e-02, -1.2058e-01, -1.1086e+00,\n",
       "         -2.3352e+00],\n",
       "        [-9.1079e-02, -1.2058e-01, -1.1086e+00, -2.3352e+00, -1.6346e-01,\n",
       "          1.3759e+00],\n",
       "        [-1.1086e+00, -2.3352e+00, -1.6346e-01,  1.3759e+00, -1.1086e+00,\n",
       "         -2.3352e+00],\n",
       "        [-9.1079e-02, -1.2058e-01, -9.1079e-02, -1.2058e-01, -9.1079e-02,\n",
       "         -1.2058e-01],\n",
       "        [-9.1079e-02, -1.2058e-01, -9.1079e-02, -1.2058e-01, -2.1194e-01,\n",
       "          6.2278e-01],\n",
       "        [-9.1079e-02, -1.2058e-01, -2.1194e-01,  6.2278e-01,  5.5838e-01,\n",
       "          1.7255e-01],\n",
       "        [-2.1194e-01,  6.2278e-01,  5.5838e-01,  1.7255e-01, -1.1086e+00,\n",
       "         -2.3352e+00],\n",
       "        [ 5.5838e-01,  1.7255e-01, -1.1086e+00, -2.3352e+00,  8.4964e-01,\n",
       "         -2.1192e-03],\n",
       "        [-1.1086e+00, -2.3352e+00,  8.4964e-01, -2.1192e-03, -6.4305e-01,\n",
       "          9.2540e-01],\n",
       "        [ 8.4964e-01, -2.1192e-03, -6.4305e-01,  9.2540e-01, -1.2148e+00,\n",
       "         -1.1060e-01],\n",
       "        [-6.4305e-01,  9.2540e-01, -1.2148e+00, -1.1060e-01, -1.2148e+00,\n",
       "         -1.1060e-01],\n",
       "        [-1.2148e+00, -1.1060e-01, -1.2148e+00, -1.1060e-01, -1.1086e+00,\n",
       "         -2.3352e+00],\n",
       "        [-9.1079e-02, -1.2058e-01, -9.1079e-02, -1.2058e-01, -9.1079e-02,\n",
       "         -1.2058e-01],\n",
       "        [-9.1079e-02, -1.2058e-01, -9.1079e-02, -1.2058e-01,  5.5838e-01,\n",
       "          1.7255e-01],\n",
       "        [-9.1079e-02, -1.2058e-01,  5.5838e-01,  1.7255e-01,  2.3154e+00,\n",
       "          5.1048e-01],\n",
       "        [ 5.5838e-01,  1.7255e-01,  2.3154e+00,  5.1048e-01,  7.0854e-01,\n",
       "         -9.1979e-01],\n",
       "        [ 2.3154e+00,  5.1048e-01,  7.0854e-01, -9.1979e-01, -1.7234e+00,\n",
       "          6.2434e-02],\n",
       "        [ 7.0854e-01, -9.1979e-01, -1.7234e+00,  6.2434e-02, -2.1194e-01,\n",
       "          6.2278e-01],\n",
       "        [-1.7234e+00,  6.2434e-02, -2.1194e-01,  6.2278e-01, -1.1086e+00,\n",
       "         -2.3352e+00]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.view(32, 6) # this does the concatenation that we have done previously but it is more efficient because it does not\n",
    "# copy memory instead it just interprets what's in the memory differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # for -1 here pytorch infers what should go into that dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(emb.view(-1, 6) @ W1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will broadcasting allow emb.view(-1, 6) @ W1 + b1 ?? \n",
    "# 32, 100\n",
    "#   1, 100 --> when adding this to 32,100 tensor this will get copied 32 times and add to all the rows\n",
    "# so this is correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((100,27)) # 27 for 27 possible characters that will come next\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = h @ W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = logits.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = counts / counts.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(18.6424)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- now made respectable :) ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.Size([32]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape # dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C = torch.randn((27,2), generator=g)\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X] # (32, 3, 2)\n",
    "h = torch.tanh(emb.view(-1,6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "# Never do below three lines in production instead use F.cross_entropy()\n",
    "# counts = logits.exp()\n",
    "# prob = counts / counts.sum(1, keepdims=True)\n",
    "# loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.\n",
    "# counts = logits.exp()\n",
    "# prob = counts / counts.sum(1, keepdims=True)\n",
    "# loss = -prob[torch.arange(32), Y].log().mean()\n",
    "\n",
    "# 2.\n",
    "# F.cross_entropy(logits, Y)\n",
    "#\n",
    "# 1 and 2. are equivalent. 2. is more efficient under the hood because instead of creating separate tensors\n",
    "# for each equation it does a fused kernel.\n",
    "# a. the forward pass is much more efficient because of reason stated above\n",
    "# b. the backward pass will be much more efficient because the derivative will be simplified\n",
    "# c. the numbers are better well behaved. logits.exp() can cause nans but pytorch internally solved this\n",
    "#    by subtracting the max number from the tensor which still gives an equivalent output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25361168384552\n"
     ]
    }
   ],
   "source": [
    " for _ in range(1000):\n",
    "        \n",
    "    # forward pass\n",
    "    emb = C[X] # (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max values along dim 0: tensor([3, 4])\n",
      "Indices of max values along dim 0: tensor([1, 1])\n",
      "Max values along dim 1: tensor([2, 4])\n",
      "Indices of max values along dim 1: tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "# playing with the tensor.max() for my understanding\n",
    "x = torch.tensor([[2, 1], [3, 4]])\n",
    "\n",
    "max_values, max_indices = x.max(dim=0)\n",
    "\n",
    "print(\"Max values along dim 0:\", max_values)\n",
    "print(\"Indices of max values along dim 0:\", max_indices)\n",
    "\n",
    "x = torch.tensor([[2, 1], [3, 4]])\n",
    "\n",
    "max_values, max_indices = x.max(dim=1)\n",
    "\n",
    "print(\"Max values along dim 1:\", max_values)\n",
    "print(\"Indices of max values along dim 1:\", max_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([13.8819, 18.9210, 21.1106, 21.3680, 17.5255, 13.8819, 16.7710, 14.8131,\n",
       "        16.5614, 19.2740, 16.8058, 21.7451, 13.8819, 18.0764, 18.0122, 20.9834,\n",
       "        13.8819, 17.3934, 16.1381, 18.0022, 19.2822, 16.8741, 11.6843, 11.3966,\n",
       "        16.0696, 13.8819, 16.8989, 17.7183, 13.3022, 16.8211, 20.0219, 17.1322],\n",
       "       grad_fn=<MaxBackward0>),\n",
       "indices=tensor([19, 13, 13,  1,  0, 19, 12,  9, 22,  9,  1,  0, 19, 22,  1,  0, 19, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at logits.max and compare against Y to see why loss can't be zero\n",
    "# loss can't be zero because we have ... in the beginning and they map to completely different values most of the time.\n",
    "logits.max(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  ..., 26, 24,  0])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- now made respectable :) ---------------- cleaned up version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.Size([228146]))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape # dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C = torch.randn((27,2), generator=g)\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "lre = torch.linspace(-3, 0, 1000) # learning rate exponent\n",
    "lrs = 10**lre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.668642044067383\n",
      "19.547134399414062\n",
      "20.24677085876465\n",
      "21.31930160522461\n",
      "16.994714736938477\n",
      "16.058204650878906\n",
      "19.999771118164062\n",
      "18.43269157409668\n",
      "22.727069854736328\n",
      "18.27863311767578\n",
      "21.136085510253906\n",
      "15.871482849121094\n",
      "20.224401473999023\n",
      "18.966400146484375\n",
      "19.36940574645996\n",
      "21.601890563964844\n",
      "21.991422653198242\n",
      "19.3601016998291\n",
      "18.9153995513916\n",
      "17.814815521240234\n",
      "17.472797393798828\n",
      "22.65011978149414\n",
      "18.823829650878906\n",
      "20.27212142944336\n",
      "20.813501358032227\n",
      "17.798551559448242\n",
      "19.318870544433594\n",
      "19.346134185791016\n",
      "17.018573760986328\n",
      "17.049057006835938\n",
      "18.64921760559082\n",
      "20.202402114868164\n",
      "20.557998657226562\n",
      "18.127859115600586\n",
      "19.86014747619629\n",
      "19.513843536376953\n",
      "19.1051025390625\n",
      "17.123342514038086\n",
      "21.553592681884766\n",
      "20.52910041809082\n",
      "17.29924774169922\n",
      "18.37529754638672\n",
      "17.90870475769043\n",
      "17.13455581665039\n",
      "15.718107223510742\n",
      "16.671146392822266\n",
      "18.89324188232422\n",
      "17.906776428222656\n",
      "18.897478103637695\n",
      "16.95439910888672\n",
      "18.371116638183594\n",
      "20.275789260864258\n",
      "18.368051528930664\n",
      "18.17740249633789\n",
      "19.708791732788086\n",
      "19.706829071044922\n",
      "16.610252380371094\n",
      "18.91852378845215\n",
      "16.67423439025879\n",
      "18.98002815246582\n",
      "17.185943603515625\n",
      "15.84752082824707\n",
      "19.190860748291016\n",
      "16.74451446533203\n",
      "19.758642196655273\n",
      "14.661848068237305\n",
      "18.10577964782715\n",
      "18.121440887451172\n",
      "18.30415916442871\n",
      "19.28837013244629\n",
      "18.006256103515625\n",
      "17.8405704498291\n",
      "17.149856567382812\n",
      "20.556669235229492\n",
      "17.149383544921875\n",
      "15.867475509643555\n",
      "19.880699157714844\n",
      "21.20709800720215\n",
      "15.311657905578613\n",
      "17.133811950683594\n",
      "19.92754554748535\n",
      "19.535118103027344\n",
      "16.405391693115234\n",
      "18.037229537963867\n",
      "14.05589485168457\n",
      "16.035791397094727\n",
      "19.560680389404297\n",
      "16.775543212890625\n",
      "18.401884078979492\n",
      "15.892901420593262\n",
      "15.3178129196167\n",
      "17.024307250976562\n",
      "17.481870651245117\n",
      "20.1850528717041\n",
      "16.228214263916016\n",
      "14.97284984588623\n",
      "18.277070999145508\n",
      "17.78712272644043\n",
      "17.89476776123047\n",
      "15.001401901245117\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[X[ix]] # (num_of_examples, 3, 2) -> (32, 3, 2) b/c of minibatching\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    print(loss.item())\n",
    "   \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data += -1 * p.grad\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5576, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate loss for all of x and all of y\n",
    "emb = C[X] # (num_of_examples, 3, 2)\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 2])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.shape "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
